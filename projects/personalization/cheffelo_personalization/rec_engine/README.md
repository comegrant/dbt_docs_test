# Recommendation

## What
This project tries to figure out how good a recipe will be liked by a customer.

Therefore, the project tries to score how "likable" a recipe is for our users, but we also rank the recipes based on an available week menu. Leading to the need of creating enough variation in our recipes as well.

To view how the data flows. Look at the [data flow](/docs/README.md).

## Why
We want to provide a personalised expereience to our customers to improve the user experience. Furthermore, this can make it possible to remove the concepts that we currently have. As we can generate a personalised meal kit based on the customers preferences.

## How
The recommendations are generated by a content based model to score each recipe, combined with a KMeans clustering model to ensure some variability in the dishes.

Therefore, we combine the two models and use a round-robin ish. approach where we loop through each cluster, and pick the best scoring recipe in that cluster.


## Who use it?
Currently the recommendations are used in the following places.

### Frontend
We expose the top 8 recipes within a given menu (year-week), and present them in the "we recommend" tab when picking recipes our self.
This is the [presented recommendations model](/docs/models/presented_recommendations/README.md).

### Meal selector
The recommendations are also used in the meal selector, to replace unwanted dishes with something that could be more appropeate. Here will the whole ranking of each menu week.
Reads from [rec engine model](/docs/models/rec_engine/README.md).

### Pre-selector
Similar to the meal selector. However, rather then replacing meals will we generate a full meal kit. Therefore, recommendations will be one part of the algorithm.
Reads from [rec engine model](/docs/models/rec_engine/README.md).

## How often is it run?
As of this writing do a [ADF](https://adf.azure.com/en/authoring/pipeline/TrainPredictRecommendation?factory=/subscriptions/7c54c7c3-c54c-44bd-969c-440ecef1d917/resourceGroups/gg-analytics-tools-prod/providers/Microsoft.DataFactory/factories/BrandhubDataFactory) pipeline exist with 4 different triggers.

Each trigger runs the recommendation engine weekly but for different companies.

## Run the recommendation pipeline

### Env vars
To run the pipeline, you need access to the data lake where the features are read from.
Therefore, adding the following envs are needed:

```env
DATALAKE_SERVICE_ACCOUNT_NAME="gganalyticsdatalake"
DATALAKE_STORAGE_ACCOUNT_KEY="..."
```

Furthermore, if you want to compute using the `CompanyDataset`, you will also need to define the ADB connection. Here is it mainly the UID, and PWD that needs to be defined in the string bellow.

```env
ADB_CONNECTION="PWD=MY_PASSWORD;UID=MY_USER_NAME;DRIVER=ODBC Driver 18 for SQL Server;DATABASE=AnalyticsDB;SERVER=gg-analytics.database.windows.net;PORT=1433"
```

Bellow are different ways of running the recommenations show.
Mainly through a shell command. However, it is possilbe to trigger through Python as well.

### Run training and prediction
The pipeline can be ran with the following shell command.

```bash
python -m cheffelo_personalization.rec_engine.main \
    --manual-year-week 202344 202344 202344 202344 202344 \
    --manual-recipe-ids 64881 64882 64883 64884 64885 \
    --manual-product-ids abc bdb yyy zzz uuu \
    --write-to data/rec_engine
```

Here do we define which recipe ids to train based on, and which to predict for. While also defining where to write the results.

This is mainly ment to be used for debugging purposes.

### Production run

We can also train and predict for a given company id, and the meny year weeks as shown bellow. Since we do not define a `--write-to` flag will we write all ML outputs to the defined dir.

```bash
python -m cheffelo_personalization.rec_engine.main \
    --company-id 6A2D0B60-84D6-4830-9945-58D518D27AC2 \
    --year-weeks 202344 202345 202346
```

### Run through Python
To run the recommendations through python. Use something like the following code.

```python
from datetime import timedelta
from cheffelo_personalization.rec_engine.run import run, DynamicDataset
from aligned import FeatureStore
import logging

# Only log errors from the azure module.
# It can create a log of noise when connecting to the data lake
logging.getLogger("azure.core.pipeline.policies.http_logging_policy").setLevel(
  logging.ERROR
)

store = await FeatureStore.from_dir(".")
dataset = DynamicDataset(
  company_id="09ECD4F0-AE58-4539-8E8F-9275B1859A19",
  year_weeks=[202348, 202349, 202350, 202351, 202352]
)

await run(
  dataset=dataset,
  store=store,
  update_source_threshold=None, # makes sure we do not update the source tables.
  write_to_path="output/to/local/folder/",
)
```
