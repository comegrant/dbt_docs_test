{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_tagging.db import get_serverless_spark_session\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(BaseModel):\n",
    "    language: Literal[\"danish\", \"norwegian\", \"swedish\"]\n",
    "    env: Literal[\"dev\", \"test\", \"prod\"]\n",
    "\n",
    "    @property\n",
    "    def language_id(self) -> int:\n",
    "        if self.language == \"norwegian\":\n",
    "            return 1\n",
    "        elif self.language == \"swedish\":\n",
    "            return 5\n",
    "        elif self.language == \"danish\":\n",
    "            return 6\n",
    "        else:\n",
    "            raise ValueError(\"Invalid language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(language=\"norwegian\", env=\"prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(language=\"norwegian\", env=\"prod\")\n",
    "\n",
    "language_id = 1 if args.language == \"norwegian\" else 5 if args.language == \"swedish\" else 6\n",
    "spark = get_serverless_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_tagging.predict.mappings import cuisine_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from recipe_tagging.predict.mappings import cuisine_mapping\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\n",
    "    f\"\"\"\n",
    "    with recipes as ( \n",
    "    select\n",
    "        pk_dim_recipes\n",
    "        , main_recipe_id\n",
    "        , recipe_id\n",
    "        , recipe_name\n",
    "        , recipe_main_ingredient_id\n",
    "        , recipe_main_ingredient_name_local\n",
    "        , recipe_difficulty_level_id\n",
    "        , cooking_time_from\n",
    "        , cooking_time_to\n",
    "        , language_id\n",
    "        , is_in_recipe_universe\n",
    "    from gold.dim_recipes dr\n",
    "    ),\n",
    "\n",
    "    taxonomies as (\n",
    "        select\n",
    "            pk_dim_taxonomies\n",
    "            , taxonomy_id\n",
    "            , language_id\n",
    "            , taxonomy_name_local\n",
    "        from gold.dim_taxonomies\n",
    "        where taxonomy_type_name not like 'seo_%'\n",
    "    ),\n",
    "\n",
    "    taxonomy_list as (\n",
    "        select\n",
    "            recipes.recipe_id\n",
    "            , concat_ws(', ', collect_list(taxonomies.taxonomy_name_local)) as taxonomy_name_list\n",
    "            , concat_ws(', ', collect_list(taxonomies.taxonomy_id)) as taxonomy_id_list\n",
    "            , size(collect_set(taxonomies.taxonomy_name_local)) as number_of_taxonomies\n",
    "        from recipes\n",
    "        left join gold.bridge_dim_recipes_dim_taxonomies as bridge\n",
    "            on recipes.pk_dim_recipes = bridge.fk_dim_recipes\n",
    "        left join taxonomies\n",
    "            on bridge.fk_dim_taxonomies = taxonomies.pk_dim_taxonomies\n",
    "        group by 1\n",
    "    ),\n",
    "\n",
    "    generic_ingredients as (\n",
    "        select\n",
    "            portions.recipe_id\n",
    "            , ingredients.generic_ingredient_id\n",
    "            , ingredient_translations.generic_ingredient_name\n",
    "            , ingredient_translations.language_id\n",
    "        from silver.pim__recipe_portions as portions\n",
    "        left join silver.pim__chef_ingredient_sections as sections\n",
    "            on portions.recipe_portion_id = sections.recipe_portion_id\n",
    "        left join silver.pim__chef_ingredients as ingredients\n",
    "            on\n",
    "                sections.chef_ingredient_section_id\n",
    "                = ingredients.chef_ingredient_section_id\n",
    "        left join silver.pim__generic_ingredient_translations as ingredient_translations\n",
    "            on ingredients.generic_ingredient_id = ingredient_translations.generic_ingredient_id\n",
    "        where ingredients.generic_ingredient_id is not null\n",
    "    ),\n",
    "\n",
    "    generic_ingredient_list as (\n",
    "        select\n",
    "        recipes.recipe_id\n",
    "        , concat_ws(', ', collect_set(generic_ingredients.generic_ingredient_name)) as generic_ingredient_name_list\n",
    "        , concat_ws(', ', collect_set(generic_ingredients.generic_ingredient_id)) as generic_ingredient_id_list\n",
    "        , size(collect_set(generic_ingredients.generic_ingredient_id)) as number_of_ingredients\n",
    "        from recipes\n",
    "        left join generic_ingredients\n",
    "            on recipes.recipe_id = generic_ingredients.recipe_id\n",
    "            and recipes.language_id = generic_ingredients.language_id\n",
    "            group by 1\n",
    "    ),\n",
    "\n",
    "    recipe_allergens as (\n",
    "        select\n",
    "            recipes.recipe_id\n",
    "            , dap.preference_name_combinations\n",
    "            , dap.allergen_name_combinations\n",
    "            , dap.taste_name_combinations_excluding_allergens\n",
    "        from recipes\n",
    "        left join intermediate.int_recipe_preferences_unioned irp\n",
    "            on recipes.recipe_id = irp.recipe_id\n",
    "        left join gold.dim_all_preference_combinations dap\n",
    "            on irp.preference_combination_id = dap.pk_preference_combination_id\n",
    "    ),\n",
    "\n",
    "    nutritional_info as (\n",
    "        select\n",
    "            recipe_id\n",
    "            , recipe_portion_id\n",
    "            , portion_size\n",
    "            , protein_gram_per_portion\n",
    "            , carbs_gram_per_portion\n",
    "            , fat_gram_per_portion\n",
    "            , sat_fat_gram_per_portion\n",
    "            , sugar_gram_per_portion\n",
    "            , sugar_added_gram_per_portion\n",
    "            , fiber_gram_per_portion\n",
    "            , salt_gram_per_portion\n",
    "            , fg_fresh_gram_per_portion\n",
    "            , fg_proc_gram_per_portion\n",
    "            , total_kcal_per_portion\n",
    "        from mlgold.ml_recipe_nutritional_facts\n",
    "        where portion_size = 4\n",
    "    )\n",
    "\n",
    "    select\n",
    "        recipes.recipe_id\n",
    "        , recipes.recipe_name\n",
    "        , recipes.recipe_main_ingredient_name_local\n",
    "        , recipes.language_id\n",
    "        , recipes.cooking_time_to\n",
    "        , taxonomy_list.taxonomy_name_list\n",
    "        , generic_ingredient_list.generic_ingredient_name_list\n",
    "        , recipe_allergens.preference_name_combinations\n",
    "        , case \n",
    "        when nutritional_info.total_kcal_per_portion < \n",
    "        case \n",
    "            when recipes.language_id = 1 then 750 --norway\n",
    "            when recipes.language_id = 5 then 550 -- sweden\n",
    "            when recipes.language_id = 6 then 600 --denmark\n",
    "        end\n",
    "        and (nutritional_info.fg_fresh_gram_per_portion+nutritional_info.fg_proc_gram_per_portion) > 150 then true \n",
    "        else false\n",
    "        end as is_low_calorie\n",
    "        , case when nutritional_info.fiber_gram_per_portion > 10 then true else false end as is_high_fiber\n",
    "        , case when nutritional_info.fat_gram_per_portion < (total_kcal_per_portion*0.3/9) then true else false end as is_low_fat\n",
    "        , case when nutritional_info.sugar_gram_per_portion < (total_kcal_per_portion*0.07/4) then true else false end as is_low_sugar\n",
    "    from recipes\n",
    "    left join taxonomy_list\n",
    "        on recipes.recipe_id = taxonomy_list.recipe_id\n",
    "    left join generic_ingredient_list\n",
    "        on recipes.recipe_id = generic_ingredient_list.recipe_id\n",
    "    left join recipe_allergens\n",
    "        on recipes.recipe_id = recipe_allergens.recipe_id\n",
    "    left join nutritional_info\n",
    "        on recipes.recipe_id = nutritional_info.recipe_id\n",
    "    where recipes.is_in_recipe_universe = false\n",
    "    and recipes.language_id = {language_id}\n",
    "    order by recipes.recipe_id desc\n",
    "    limit 15000\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norwegian cuisine related taxonomies\n",
    "cuisine_keywords_norwegian = {\n",
    "    'African': ['Afrika'],\n",
    "    'American': ['Amerikansk Bbq', 'Amerika'],\n",
    "    'Arabian': ['Marocco'],\n",
    "    'Asian': ['Asia'],\n",
    "    'British': ['England'],\n",
    "    'Chinese': ['Kina'],\n",
    "    'Danish': ['Tyskland'],\n",
    "    'French': ['Frankrike'],\n",
    "    'Fusion Food': ['Fusion'],\n",
    "    'Greek': ['Hellas'],\n",
    "    'Hawaiian': [],\n",
    "    'Indian': ['India'],\n",
    "    'Italian': ['Italia'],\n",
    "    'Japanese': ['Japan'],\n",
    "    'Korean': ['Korea'],\n",
    "    'Lebanese': ['Libanon'],\n",
    "    'Mediterranean': ['Middelhavet'],\n",
    "    'Mexican': ['Mexico'],\n",
    "    'Middle Eastern': ['Midtøsten'],\n",
    "    'Norwegian': ['Nordisk', 'Husmannskost'],\n",
    "    'Persian': [],\n",
    "    'South American': ['Sør- Amerika', 'Karibien', 'Karibia', 'Latin- Amerika'],\n",
    "    'Spanish': ['Paella', 'Spania', 'Portugal'],\n",
    "    'Swedish': [],\n",
    "    'Tex-mex': ['Tex Mex '],\n",
    "    'Thai': ['Thai', 'Thailand'],\n",
    "    'Vietnamese': ['Vietnam']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swedish cuisine related taxonomies\n",
    "cuisine_keywords_swedish = {\n",
    "    'African': ['Afrika'],\n",
    "    'American': ['American Bbq', 'Amerika', 'Kalifornien'],\n",
    "    'Arabian': ['Marocko'],\n",
    "    'Asian': ['Asiatisk', 'Asian', 'Asien'],\n",
    "    'British': ['England'],\n",
    "    'Chinese': ['Kina'],\n",
    "    'Danish': ['Danska'],\n",
    "    'French': ['Frankrike'],\n",
    "    'Fusion Food': ['Fusion'],\n",
    "    'Greek': ['Grekisk', 'Grekland', 'Grekiskt'],\n",
    "    'Hawaiian': [],\n",
    "    'Indian': ['Indian', 'Indien'],\n",
    "    'Italian': ['Italien'],\n",
    "    'Japanese': ['Japan'],\n",
    "    'Korean': ['Korea'],\n",
    "    'Lebanese': ['Libanon'],\n",
    "    'Mediterranean': ['Mediterranean', 'Medelhavet', 'Medelhav'],\n",
    "    'Mexican': ['Mexiko', 'Mexikanskt'],\n",
    "    'Middle Eastern': ['Middle Eastern', 'Mellanöstern'],\n",
    "    'Norwegian': ['Norska'],\n",
    "    'Persian': ['Persisk', 'Persiska'],\n",
    "    'South American': ['Sydamerika', 'Latinamerika', 'Karibien'],\n",
    "    'Spanish': ['Spanien', 'Paella'],\n",
    "    'Swedish': ['Husmanskost', 'Nordisk'],\n",
    "    'Tex-mex': ['Tex Mex', 'Tex-mex'],\n",
    "    'Thai': ['Thailand', 'Thai'],\n",
    "    'Vietnamese': ['Vietnam']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# danish cuisine related taxonomies\n",
    "cuisine_keywords_danish = {\n",
    "    'African': ['Afrika'],\n",
    "    'American': ['Usa', 'American Bbq', 'Californien'],\n",
    "    'Arabian': ['Marokko'],\n",
    "    'Asian': ['Asiatisk Mad', 'Asien', 'Asian'],\n",
    "    'British': ['England'],\n",
    "    'Chinese': ['Kina'],\n",
    "    'Danish': ['Husmandskost', 'Nordisk', 'Tyskland'],\n",
    "    'French': ['Frankrig'],\n",
    "    'Fusion Food': ['Fusion'],\n",
    "    'Greek': ['Grækenland'],\n",
    "    'Hawaiian': [],\n",
    "    'Indian': ['Indisk Mad', 'Indien'],\n",
    "    'Italian': ['Italien'],\n",
    "    'Japanese': ['Japan'],\n",
    "    'Korean': ['Korea'],\n",
    "    'Lebanese': ['Libanon'],\n",
    "    'Mediterranean': ['Middelhavet', 'Middelhav', 'Mediterranean'],\n",
    "    'Mexican': ['Mexicansk Mad', 'Mexico'],\n",
    "    'Middle Eastern': ['Mellemøsten'],\n",
    "    'Norwegian': [],\n",
    "    'Persian': [],\n",
    "    'South American': ['Sydamerika', 'Caribien'],\n",
    "    'Spanish': ['Spanien', 'Portugal', 'Paella'],\n",
    "    'Swedish': [],\n",
    "    'Tex-mex': ['Tex Mex'],\n",
    "    'Thai': ['Thailand'],\n",
    "    'Vietnamese': ['Vietnam']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snorkel labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUISINE_LABELS = {}\n",
    "i = 0\n",
    "for key in cuisine_mapping.keys():\n",
    "    CUISINE_LABELS[key] = i\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_lookup(x, keywords):\n",
    "    x_lower = x.lower()\n",
    "    return any(keyword.lower() in x_lower for keyword in keywords)\n",
    "\n",
    "def make_keyword_lf(label, keywords, fields=('recipe_name', 'taxonomy_name_list')):\n",
    "    def lf(x):\n",
    "        for field in fields:\n",
    "            if keyword_lookup(getattr(x, field), keywords):\n",
    "                return label\n",
    "        return ABSTAIN\n",
    "    return LabelingFunction(name=f\"lf_{label}\", f=lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = []\n",
    "\n",
    "for key, _ in cuisine_mapping.items():\n",
    "    lfs.append(make_keyword_lf(CUISINE_LABELS[key], cuisine_keywords_norwegian[key] + cuisine_mapping[key][language_id])) # MAKE LANGUGAE SPECIFIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df)\n",
    "label_model = LabelModel(cardinality=len(CUISINE_LABELS), verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=100, seed=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = label_model.predict(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weak_label\"] = label_model.predict(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUISINE_LABELS_REVERSED = {v: k for k, v in CUISINE_LABELS.items()}\n",
    "df[\"cuisine_name\"] = df[\"weak_label\"].map(CUISINE_LABELS_REVERSED).fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cuisine_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all duplicated recipe_names from df\n",
    "df = df.drop_duplicates(subset=['recipe_name', 'cuisine_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cuisine_name'].str.contains('Swedish')][['recipe_name', 'cuisine_name']].head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exact mapped labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, _ in cuisine_mapping.items():\n",
    "    print(key)\n",
    "    print(list(map(lambda x: x.lower(), cuisine_keywords_danish[key] + cuisine_mapping[key][language_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cuisine_name'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    matched_cuisines = set()\n",
    "    recipe_name = row['recipe_name'].lower()\n",
    "    taxonomy_list = row['taxonomy_name_list'].lower()\n",
    "    \n",
    "    for key in cuisine_mapping.keys():\n",
    "        keywords = [k.lower() for k in (cuisine_keywords_danish[key] + cuisine_mapping[key][language_id])]\n",
    "        if any(keyword in recipe_name for keyword in keywords):\n",
    "            matched_cuisines.add(key)\n",
    "        if any(keyword in taxonomy_list for keyword in keywords):\n",
    "            matched_cuisines.add(key)\n",
    "    \n",
    "    df.loc[index, 'cuisine_name'] = ', '.join(matched_cuisines) if matched_cuisines else df.loc[index, 'cuisine_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.cuisine_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst[tst['cuisine_name'].str.contains('British, Norwegian')][['recipe_name', 'cuisine_name']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cuisine(cuisine_str):\n",
    "    specific_asian_cuisines = {'Indian', 'Japanese', 'Chinese', 'Vietnamese', 'Korean', 'Thai'}\n",
    "    specific_mediterranean_cuisines = {'Greek', 'Spanish', 'French', 'Italian'}\n",
    "    specific_fusion = {'Fusion Food'}\n",
    "    specific_african = {'Lebanese', 'Persian'}\n",
    "    cuisines = [c.strip() for c in cuisine_str.split(',')]\n",
    "\n",
    "    if len(cuisines) >= 4:\n",
    "        return 'Fusion Food'\n",
    "    \n",
    "    for cuisine in specific_african:\n",
    "        if cuisine in cuisines:\n",
    "            return cuisine\n",
    "    \n",
    "    for cuisine in specific_fusion:\n",
    "        if cuisine in cuisines:\n",
    "            return cuisine\n",
    "\n",
    "    for cuisine in specific_asian_cuisines:\n",
    "        if cuisine in cuisines:\n",
    "            return cuisine\n",
    "    \n",
    "    for cuisine in specific_mediterranean_cuisines:\n",
    "        if cuisine in cuisines:\n",
    "            return cuisine\n",
    "    \n",
    "    return ', '.join(cuisines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst['cuisine_name'] = tst['cuisine_name'].apply(clean_cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where cuisine_name is empty\n",
    "tst = tst[tst['cuisine_name'] != '']\n",
    "#drop rows where cuisine name has value count < 10\n",
    "cuisine_counts = tst['cuisine_name'].value_counts()\n",
    "tst = tst[tst['cuisine_name'].isin(cuisine_counts[cuisine_counts >= 10].index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = tst.drop_duplicates(subset=['recipe_name', 'cuisine_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode tst on cuisine_name (split on comma)\n",
    "tst['cuisine_name'] = tst['cuisine_name'].str.split(', ')\n",
    "tst = tst.explode('cuisine_name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tst.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where cuisine_name is Unknown\n",
    "df = df[df['cuisine_name'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients'] = df['generic_ingredient_name_list'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(x):\n",
    "    x = [\" \".join([WordNetLemmatizer().lemmatize(q) for q in p.split()]) for p in x]\n",
    "    x = list(map(lambda x: re.sub(\"[^a-zA-ZæøåäöÆØÅÄÖ]\", \" \", x), x))\n",
    "    x = \" \".join(x)                                \n",
    "    x = x.lower()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients'] = df['ingredients'].apply(process_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['ingredients']\n",
    "y = df['cuisine_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(train, test=None):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        stop_words=stopwords.words(args.language),\n",
    "        ngram_range = ( 1 , 1 ),analyzer=\"word\", \n",
    "        max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n",
    "    train = tfidf.fit_transform(train)\n",
    "    if test is not None:\n",
    "        test = tfidf.transform(test)\n",
    "        return train, test, tfidf\n",
    "    else:\n",
    "        return train, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf, tfidf = tfidf_vectorizer(X)\n",
    "train = train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalfn(C, gamma):\n",
    "    s = SVC(C=float(C), gamma=float(gamma), kernel='rbf', class_weight='balanced', probability=True)\n",
    "    f = cross_val_score(s, train, y, cv=3, scoring='f1_micro')\n",
    "    return f.max()\n",
    "\n",
    "opt = BayesianOptimization(evalfn, {'C': (0.1, 1000), 'gamma': (0.0001, 1)})\n",
    "opt.maximize(n_iter=20, init_points=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=float(opt.max['params']['C']), gamma=float(opt.max['params']['gamma']), kernel='rbf', probability=True)\n",
    "clf.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "pickle.dump(clf, open(f'models/{args.language}_model_keyword.pkl', 'wb'))\n",
    "pickle.dump(tfidf, open(f'models/{args.language}_vectorizer_keyword.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
