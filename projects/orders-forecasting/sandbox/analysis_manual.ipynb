{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_yaml\n",
    "from paths import CONFIG_DIR\n",
    "company = \"GL\"\n",
    "company_configs = read_yaml(\n",
    "    file_name=\"company_configs\",\n",
    "    directory=CONFIG_DIR\n",
    ")\n",
    "company_config = company_configs[company]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import itertools\n",
    "from lmkgroup_ds_utils.azure.storage import BlobConnector\n",
    "\n",
    "def get_file_list(\n",
    "    datalake_handler: BlobConnector,\n",
    "    file_prefix: str,\n",
    "    file_suffix: str,\n",
    "    path: Optional[str] = \"forecasting_pipelines/manual_forecast/archive\",\n",
    ")-> list:\n",
    "    blob_list = datalake_handler.list_blobs(\n",
    "        container=\"data-science\", path=path\n",
    "    )\n",
    "\n",
    "    blob_name_splitted = [a_blob.split(\"/\") for a_blob in blob_list]\n",
    "    # Flatten the list, and keep only unique values\n",
    "    unique_version_names_list = list(set(list(itertools.chain(*blob_name_splitted))))\n",
    "    # If folder name starts with the company code, or if its latest\n",
    "    file_list = [\n",
    "        file_name\n",
    "        for file_name in unique_version_names_list\n",
    "        if (file_name.startswith(file_prefix)) & (file_name.endswith(file_suffix))\n",
    "    ]\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(\n",
    "    datalake_handler: BlobConnector,\n",
    "    file_directory: str,\n",
    "    file_prefix: str,\n",
    "    file_suffix: Optional[str] = \".csv\",\n",
    "    container_url: Optional[str] = \"https://gganalyticsdatalake.blob.core.windows.net/data-science\"\n",
    "):\n",
    "    file_names = get_file_list(\n",
    "        datalake_handler=datalake_handler,\n",
    "        file_prefix=file_prefix,\n",
    "        file_suffix=file_suffix,\n",
    "        path=file_directory,\n",
    "    )\n",
    "    df_list = []\n",
    "    for a_filename in file_names:\n",
    "        print(f\"Downloading {a_filename}...\")\n",
    "        blob_url = f\"{container_url}/{file_directory}/{a_filename}\"\n",
    "        df = datalake_handler.download_csv_to_df(\n",
    "            url=blob_url\n",
    "        )\n",
    "        df_list.append(df)\n",
    "    return df_list, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def download_manual_forecast(\n",
    "    company: str,\n",
    "    datalake_handler: BlobConnector\n",
    ") -> pd.DataFrame:\n",
    "    manual_forecast_dir = \"forecasting_pipelines/manual_forecast/archive\"\n",
    "    df_list_manual, file_names_manual = download_csv(\n",
    "        datalake_handler=datalake_handler,\n",
    "        file_prefix=company,\n",
    "        file_suffix=\".csv\",\n",
    "        file_directory=manual_forecast_dir,\n",
    "    )\n",
    "\n",
    "    for df, file_name in zip(df_list_manual, file_names_manual):\n",
    "        try:\n",
    "            timestamp = file_name[-23:-4]\n",
    "            df[\"pred_timestamp\"] = timestamp\n",
    "            df[\"pred_timestamp\"] = pd.to_datetime(df[\"pred_timestamp\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df_manual = pd.concat(df_list_manual)\n",
    "    return df_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def download_ml_forecast(\n",
    "    company: str,\n",
    "    datalake_handler: BlobConnector,\n",
    ") -> pd.DataFrame:\n",
    "    ml_dir = f\"forecasting_ml/orders/predictions/{company}/\"\n",
    "    df_list_manual, file_names_manual = download_csv(\n",
    "        datalake_handler=datalake_handler,\n",
    "        file_prefix=\"pred_final\",\n",
    "        file_suffix=\".csv\",\n",
    "        file_directory=ml_dir,\n",
    "    )\n",
    "    df_list = []\n",
    "    for df, file_name in zip(df_list_manual, file_names_manual):\n",
    "        try:\n",
    "            df_list.append(df)\n",
    "            timestamp = file_name[25:-4]\n",
    "            df[\"pred_timestamp\"] = timestamp\n",
    "            df[\"pred_timestamp\"] = pd.to_datetime(df[\"pred_timestamp\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df_ml = pd.concat(df_list)\n",
    "    return df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download historical forecasts\n",
    "datalake_handler = BlobConnector(\n",
    "    local=True,\n",
    ")\n",
    "\n",
    "df_ml = download_ml_forecast(\n",
    "    company=company,\n",
    "    datalake_handler=datalake_handler\n",
    ")\n",
    "\n",
    "df_manual = download_manual_forecast(\n",
    "    company=company,\n",
    "    datalake_handler=datalake_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmkgroup_ds_utils.db.connector import DB\n",
    "\n",
    "from paths import SQL_DIR\n",
    "from utils import fetch_data_from_sql\n",
    "\n",
    "read_db = DB(\n",
    "    local=True,\n",
    "    db_name=\"analytics_db\",\n",
    ")\n",
    "\n",
    "# Get actual orders: the truth\n",
    "df_order_history = fetch_data_from_sql(\n",
    "    read_db=read_db,\n",
    "    sql_name=\"orders\",\n",
    "    directory=SQL_DIR,\n",
    "    company_id=company_config[\"company_id\"],\n",
    "    min_year=company_config[\"min_year\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to map variation id with product type\n",
    "variation_prod_type_query = \"\"\"\n",
    "    WITH menus AS (\n",
    "        SELECT\n",
    "            menu_id,\n",
    "            weekly_menus_id,\n",
    "            product_type_id\n",
    "        FROM pim.menus\n",
    "    ),\n",
    "\n",
    "    menu_variations AS (\n",
    "        SELECT\n",
    "            menu_id,\n",
    "            menu_variation_ext_id AS variation_id,\n",
    "            menu_number_days,\n",
    "            portion_id\n",
    "        FROM pim.menu_variations\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        variation_id,\n",
    "        product_type_id\n",
    "    FROM\n",
    "        menu_variations\n",
    "    LEFT JOIN\n",
    "        menus\n",
    "    ON menus.menu_id = menu_variations.menu_id\n",
    "\"\"\"\n",
    "\n",
    "df_variation_prod_type_mapping = read_db.read_data(variation_prod_type_query)\n",
    "df_variation_prod_type_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean manual forecast to separate total orders and dishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with product type id mapping\n",
    "import re\n",
    "df_manual = df_manual.merge(\n",
    "    df_variation_prod_type_mapping.drop_duplicates(),\n",
    "    on=\"variation_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_manual[[\"year\", \"week\"]] = df_manual[[\"year\", \"week\"]].astype(int)\n",
    "df_manual[\"quantity\"] = df_manual[\"quantity\"].apply(lambda x: re.sub('[^0-9\\.]','', str(x))).astype(float)\n",
    "# Calculate mealbox amount\n",
    "prod_type_mealbox = \"2F163D69-8AC1-6E0C-8793-FF0000804EB3\"\n",
    "df_manual_mealbox = df_manual[df_manual[\"product_type_id\"] == prod_type_mealbox]\n",
    "df_manual_mealbox = pd.DataFrame(\n",
    "    df_manual_mealbox.groupby([\"year\", \"week\", \"pred_timestamp\"])[\"quantity\"].sum()\n",
    ").reset_index()\n",
    "df_manual_mealbox = df_manual_mealbox.rename(\n",
    "    columns={\n",
    "        \"quantity\": \"num_mealboxes_orders_manual\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate flex amount\n",
    "df_flex = df_manual[df_manual[\"variation_id\"].str.startswith(\"1000\")].drop_duplicates()\n",
    "df_flex = df_flex[[\"year\", \"week\", \"pred_timestamp\", \"quantity\"]].rename(\n",
    "    columns={\"quantity\": \"num_dishes_orders_manual\"}\n",
    ")\n",
    "df_manual_forecast = df_manual_mealbox.merge(\n",
    "    df_flex,\n",
    "    on=[\"year\", \"week\", \"pred_timestamp\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Total orders: flex + mealboxes\n",
    "df_manual_forecast[\"pred_date\"] = pd.to_datetime(df_manual_forecast[\"pred_timestamp\"]).dt.date\n",
    "df_manual_forecast = df_manual_forecast.drop_duplicates(subset=[\"year\", \"week\", \"pred_date\"])\n",
    "df_manual_forecast[\"num_total_orders_manual\"] = (\n",
    "    df_manual_forecast[\"num_dishes_orders_manual\"]\n",
    "    + df_manual_forecast[\"num_mealboxes_orders_manual\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df_ml.rename(\n",
    "    columns={\n",
    "        \"num_dishes_orders\":\"num_dishes_orders_ml\",\n",
    "        \"num_total_orders\":\"num_total_orders_ml\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orders_forecasting.data import get_cut_off_date\n",
    "df_order_history = get_cut_off_date(\n",
    "    df=df_order_history,\n",
    "    cut_off_dow=company_config[\"cut_off_day\"],\n",
    "    year_col=\"year\",\n",
    "    week_col=\"week\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with truth, calculate error for manual forecast\n",
    "df_manual_forecast_merged = df_manual_forecast.merge(\n",
    "    df_order_history,\n",
    "    on=[\"year\", \"week\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_manual_forecast_merged[\"num_days_to_cut_off\"] = (\n",
    "    df_manual_forecast_merged[\"cut_off_date\"] - df_manual_forecast_merged[\"pred_timestamp\"]\n",
    ").dt.days\n",
    "\n",
    "df_manual_forecast_merged[\"num_days_to_cut_off\"] = df_manual_forecast_merged[\"num_days_to_cut_off\"] + 1\n",
    "df_manual_forecast_merged[\"mape_manual_total_orders\"] = (\n",
    "    abs(df_manual_forecast_merged[\"num_total_orders_manual\"] - df_manual_forecast_merged[\"num_total_orders\"]\n",
    ")/df_manual_forecast_merged[\"num_total_orders\"])\n",
    "\n",
    "df_manual_forecast_merged[\"mape_manual_dishes_orders\"] = (\n",
    "    abs(df_manual_forecast_merged[\"num_dishes_orders_manual\"] - df_manual_forecast_merged[\"num_dishes_orders\"]\n",
    ")/df_manual_forecast_merged[\"num_dishes_orders\"])\n",
    "\n",
    "df_manual_forecast_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with truth, calculate error for ML forecast\n",
    "df_ml_merged = df_ml.merge(\n",
    "    df_order_history,\n",
    "    on=[\"year\", \"week\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "df_ml_merged[\"num_days_to_cut_off\"] = (\n",
    "    df_ml_merged[\"cut_off_date\"] - pd.to_datetime(df_ml_merged[\"prediction_date\"])\n",
    ").dt.days\n",
    "df_ml_merged[\"num_days_to_cut_off\"] = df_ml_merged[\"num_days_to_cut_off\"] + 1\n",
    "\n",
    "df_ml_merged[\"mape_ml_total_orders\"] = (\n",
    "    abs(df_ml_merged[\"num_total_orders_ml\"] - df_ml_merged[\"num_total_orders\"]\n",
    ")/df_ml_merged[\"num_total_orders\"])\n",
    "\n",
    "df_ml_merged[\"mape_ml_dishes_orders\"] = (\n",
    "    abs(df_ml_merged[\"num_dishes_orders_ml\"] - df_ml_merged[\"num_dishes_orders\"]\n",
    ")/df_ml_merged[\"num_dishes_orders\"])\n",
    "df_ml_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_manual_forecast_merged[\"num_weeks_to_cut_off\"] = np.ceil(df_manual_forecast_merged[\"num_days_to_cut_off\"]/7.0)\n",
    "df_ml_merged[\"num_weeks_to_cut_off\"] = np.ceil(df_ml_merged[\"num_days_to_cut_off\"]/7.0)\n",
    "\n",
    "df_ml_merged = df_ml_merged.drop_duplicates(\n",
    "    subset=[\"year\", \"week\", \"num_weeks_to_cut_off\"]\n",
    ").sort_values(by=[\"year\", \"week\", \"num_weeks_to_cut_off\"])\n",
    "\n",
    "df_manual_forecast_merged = df_manual_forecast_merged.drop_duplicates(\n",
    "    subset=[\"year\", \"week\", \"num_weeks_to_cut_off\"]\n",
    ").sort_values(by=[\"year\", \"week\", \"num_weeks_to_cut_off\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_forecast_merged[\"yyyyww\"] = (df_manual_forecast_merged[\"year\"] * 100 + df_manual_forecast_merged[\"week\"])\n",
    "df_ml_merged[\"yyyyww\"] = (df_ml_merged[\"year\"] * 100 + df_ml_merged[\"week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_merged[\"mape_ml_total_orders\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_forecast_merged[\"mape_manual_total_orders\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_forecast_merged[\"mape_manual_dishes_orders\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_merged[\"mape_ml_dishes_orders\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_merged[df_ml_merged[\"year\"] == 2024][\"mape_ml_dishes_orders\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_forecast_merged[df_manual_forecast_merged[\"year\"] == 2024][\"mape_manual_dishes_orders\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import PROJECT_DIR\n",
    "fig_dir = PROJECT_DIR/\"notebook/graphs/error_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "error_col = \"mape_manual_total_orders\"\n",
    "df_manual_forecast_merged = df_manual_forecast_merged.sort_values(by=\"cut_off_date\")\n",
    "fig = px.line(\n",
    "    df_manual_forecast_merged,\n",
    "    x=\"cut_off_date\",\n",
    "    y=error_col,\n",
    "    color=\"num_weeks_to_cut_off\",\n",
    "    title=error_col,\n",
    "    markers=True,\n",
    "    hover_data=[\"yyyyww\", \"num_total_orders_manual\", \"num_total_orders\"]\n",
    ")\n",
    "fig.write_html(fig_dir/f\"{error_col}_{company}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "error_col = \"mape_ml_total_orders\"\n",
    "df_ml_merged = df_ml_merged[df_ml_merged[\"yyyyww\"]>=202333]\n",
    "df_ml_merged = df_ml_merged.sort_values(by=[\"num_weeks_to_cut_off\",\"cut_off_date\"])\n",
    "fig = px.line(\n",
    "    df_ml_merged,\n",
    "    x=\"cut_off_date\",\n",
    "    y=error_col,\n",
    "    color=\"num_weeks_to_cut_off\",\n",
    "    title=error_col,\n",
    "    markers=True,\n",
    "    hover_data=[\"yyyyww\", \"num_total_orders_ml\", \"num_total_orders\"]\n",
    ")\n",
    "fig.write_html(fig_dir/f\"{error_col}_{company}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "error_col = \"mape_manual_dishes_orders\"\n",
    "df_manual_forecast_merged = df_manual_forecast_merged.sort_values(by=\"cut_off_date\")\n",
    "fig = px.line(\n",
    "    df_manual_forecast_merged,\n",
    "    x=\"cut_off_date\",\n",
    "    y=error_col,\n",
    "    color=\"num_weeks_to_cut_off\",\n",
    "    title=error_col,\n",
    "    markers=True,\n",
    "    hover_data=[\"yyyyww\", \"num_dishes_orders_manual\", \"num_dishes_orders\"]\n",
    ")\n",
    "fig.write_html(fig_dir/f\"{error_col}_{company}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "error_col = \"mape_ml_dishes_orders\"\n",
    "df_ml_merged = df_ml_merged.sort_values(by=\"cut_off_date\")\n",
    "fig = px.line(\n",
    "    df_ml_merged,\n",
    "    x=\"cut_off_date\",\n",
    "    y=error_col,\n",
    "    color=\"num_weeks_to_cut_off\",\n",
    "    title=error_col,\n",
    "    markers=True,\n",
    "    hover_data=[\"yyyyww\", \"num_dishes_orders_ml\", \"num_dishes_orders\"]\n",
    ")\n",
    "fig.write_html(fig_dir/f\"{error_col}_{company}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we use other dishes forecasts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dir = f\"forecasting_ml/orders/predictions/{company}/\"\n",
    "\n",
    "df_list, file_names = download_csv(\n",
    "    datalake_handler=datalake_handler,\n",
    "    file_prefix=\"pred_num_total_orders\",\n",
    "    file_suffix=\".csv\",\n",
    "    file_directory=ml_dir,\n",
    ")\n",
    "\n",
    "df_list_pred_total_orders = []\n",
    "for df, file_name in zip(df_list, file_names):\n",
    "    df_list_pred_total_orders.append(df)\n",
    "    timestamp = file_name[-23:-4]\n",
    "    df[\"pred_timestamp\"] = timestamp\n",
    "    df[\"pred_timestamp\"] = pd.to_datetime(df[\"pred_timestamp\"])\n",
    "\n",
    "df_pred_total_orders = pd.concat(df_list_pred_total_orders)\n",
    "\n",
    "df_list, file_names = download_csv(\n",
    "    datalake_handler=datalake_handler,\n",
    "    file_prefix=\"pred_perc\",\n",
    "    file_suffix=\".csv\",\n",
    "    file_directory=ml_dir,\n",
    ")\n",
    "\n",
    "df_list_pred_perc = []\n",
    "for df, file_name in zip(df_list, file_names):\n",
    "    df_list_pred_perc.append(df)\n",
    "    timestamp = file_name[-23:-4]\n",
    "    df[\"pred_timestamp\"] = timestamp\n",
    "    df[\"pred_timestamp\"] = pd.to_datetime(df[\"pred_timestamp\"])\n",
    "\n",
    "df_pred_perc = pd.concat(df_list_pred_perc)\n",
    "\n",
    "df_list, file_names = download_csv(\n",
    "    datalake_handler=datalake_handler,\n",
    "    file_prefix=\"pred_num_dishes\",\n",
    "    file_suffix=\".csv\",\n",
    "    file_directory=ml_dir,\n",
    ")\n",
    "\n",
    "df_list_num_dishes = []\n",
    "for df, file_name in zip(df_list, file_names):\n",
    "    df_list_num_dishes.append(df)\n",
    "    timestamp = file_name[-23:-4]\n",
    "    df[\"pred_timestamp\"] = timestamp\n",
    "    df[\"pred_timestamp\"] = pd.to_datetime(df[\"pred_timestamp\"])\n",
    "\n",
    "df_num_dishes = pd.concat(df_list_num_dishes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perc = df_pred_perc[[\"year\", \"week\", \"estimation_date\", \"pred\"]].drop_duplicates(\n",
    "    subset=[\"year\", \"week\", \"estimation_date\"]\n",
    ").rename(columns={\"pred\": \"perc_pred\"})\n",
    "\n",
    "df_total_orders = df_pred_total_orders[[\"year\", \"week\", \"estimation_date\", \"pred\"]].drop_duplicates(\n",
    "    subset=[\"year\", \"week\", \"estimation_date\"]\n",
    ").rename(columns={\"pred\": \"total_orders_pred\"})\n",
    "\n",
    "df_ml_dishes_perc = df_perc.merge(\n",
    "    df_total_orders,\n",
    "    on=[\"year\", \"week\", \"estimation_date\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "df_ml_dishes_perc[\"pred\"] = df_ml_dishes_perc[\"perc_pred\"] * df_ml_dishes_perc[\"total_orders_pred\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_direct = df_num_dishes[[\"year\", \"week\", \"estimation_date\", \"pred\"]].drop_duplicates(\n",
    "    subset=[\"year\", \"week\", \"estimation_date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_direct_merged = df_ml_dishes_direct.merge(\n",
    "    df_order_history[[\"year\", \"week\", \"num_dishes_orders\"]],\n",
    "    how=\"left\"\n",
    ").sort_values(\n",
    "    by=[\"estimation_date\", \"year\", \"week\"]\n",
    ").dropna()\n",
    "\n",
    "df_ml_dishes_perc_merged = df_ml_dishes_perc.merge(\n",
    "    df_order_history[[\"year\", \"week\", \"num_dishes_orders\"]],\n",
    "    how=\"inner\"\n",
    ").sort_values(\n",
    "    by=[\"estimation_date\", \"year\", \"week\"]\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_direct_merged[\"error\"] = df_ml_dishes_direct_merged[\"pred\"] - df_ml_dishes_direct_merged[\"num_dishes_orders\"]\n",
    "df_ml_dishes_direct_merged[\"mape\"] = abs(df_ml_dishes_direct_merged[\"error\"])/df_ml_dishes_direct_merged[\"num_dishes_orders\"]\n",
    "df_ml_dishes_direct_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_perc_merged.head()\n",
    "\n",
    "df_ml_dishes_perc_merged[\"error\"] = df_ml_dishes_perc_merged[\"pred\"] - df_ml_dishes_perc_merged[\"num_dishes_orders\"]\n",
    "df_ml_dishes_perc_merged[\"mape\"] = abs(df_ml_dishes_perc_merged[\"error\"])/df_ml_dishes_perc_merged[\"num_dishes_orders\"]\n",
    "df_ml_dishes_perc_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_perc_merged[\"mape\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_direct_merged[\"mape\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_direct_merged[df_ml_dishes_direct_merged[\"year\"] == 2024][\"mape\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_dishes_perc_merged[df_ml_dishes_perc_merged[\"year\"] == 2024][\"mape\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
