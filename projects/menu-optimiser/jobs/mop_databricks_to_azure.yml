resources:
  jobs:
    mop-databricks-to-azure:
      name: mop-databricks-to-azure-${bundle.target}

      permissions:
        - group_name: data-scientists
          level: CAN_MANAGE_RUN

      parameters:
        - name: environment
          default: "${var.environment}"

      job_clusters:
        - job_cluster_key: mop-databricks-to-azure
          new_cluster:
            num_workers: 1
            spark_version: ${var.cluster_version}
            node_type_id: Standard_DS4_v2
            custom_tags:
              user: "Data scientists"
              tool: "menu-optimiser"
              env: ${bundle.target}
              managed_by: "manually"
            docker_image:
              url: "${var.docker_image_url}"
              basic_auth:
                username: "{{secrets/auth_common/docker-registry-username}}"
                password: "{{secrets/auth_common/docker-registry-password}}"

      schedule:
        quartz_cron_expression: '0 0 9 ? * *'
        timezone_id: UTC

      tasks:
        - task_key: mop_databricks_to_azure
          job_cluster_key: mop-databricks-to-azure
          spark_python_task:
            python_file: mop_databricks_to_azure.py
            parameters:
              - "--environment"
              - "{{job.parameters.environment}}"

        - task_key: mop_databricks_to_azure-fail
          depends_on:
            - task_key: mop_databricks_to_azure
          run_if: AT_LEAST_ONE_FAILED
          run_job_task:
            job_id: ${var.slack_notification_job_id} # Reads from the variable defined in the databricks.yml file
            job_parameters:
              environment: ${bundle.target} # Reads from the target environment as defined in the databricks.yml file
              header_message: ❌ Data export job for MOP failed
              body_message: Check Databricks for more details
              is_error: true
              relevant_people: agathe

        - task_key: mop_databricks_to_azure-success
          depends_on:
            - task_key: mop_databricks_to_azure
          run_if: NONE_FAILED
          run_job_task:
            job_id: ${var.slack_notification_job_id} # Reads from the variable defined in the databricks.yml file
            job_parameters:
              environment: ${bundle.target} # Reads from the target environment as defined in the databricks.yml file
              body_message: ✅ Data export job for MOP succeeded
              is_error: false
