{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dishes_forecasting.run_features import Args\n",
    "args = Args(company=\"GL\", env=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dishes_forecasting.logger_config import logger\n",
    "from dishes_forecasting.features.build_features import *\n",
    "from dishes_forecasting.inputs.get_data import *\n",
    "from constants.companies import get_company_by_code\n",
    "from dishes_forecasting.utils import read_yaml\n",
    "from dishes_forecasting.paths import CONFIG_DIR\n",
    "\n",
    "company_code = args.company\n",
    "logger.info(f\"Running feature generation step for {company_code}...\")\n",
    "company = get_company_by_code(company_code=company_code)\n",
    "company_id = company.company_id\n",
    "logger.info(\"Fetching configs for ...\")\n",
    "input_configs = read_yaml(directory=CONFIG_DIR, filename=\"inputs.yml\")\n",
    "\n",
    "input_config = input_configs[company_code]\n",
    "\n",
    "feature_configs = read_yaml(directory=CONFIG_DIR, filename=\"features.yml\")\n",
    "\n",
    "logger.info(\"Fetching data...\")\n",
    "(\n",
    "    df_flex_orders,\n",
    "    df_weekly_variations,\n",
    "    df_recipes,\n",
    "    df_recipe_ingredients,\n",
    "    df_recipe_price_ratings,\n",
    ") = get_input_data(company_id=company_id, input_config=input_config, env=args.env)\n",
    "\n",
    "logger.info(\"Creating feature dataframes...\")\n",
    "df_weekly_dishes_features, df_recipe_features = build_features(\n",
    "    df_flex_orders=df_flex_orders,\n",
    "    df_weekly_variations=df_weekly_variations,\n",
    "    df_recipes=df_recipes,\n",
    "    df_recipe_ingredients=df_recipe_ingredients,\n",
    "    df_recipe_price_ratings=df_recipe_price_ratings,\n",
    "    feature_configs=feature_configs,\n",
    "    language=company.language,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_recipe_features.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dishes_forecasting.run_features import run_features, Args\n",
    "\n",
    "args = Args(company=\"GL\", env=\"dev\")\n",
    "df_weekly_dishes_features, df_recipe_features = run_features(args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_dishes_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the table exists\n",
    "table_name = \"weekly_dishes_features\"\n",
    "if spark.catalog.tableExists(table_name):\n",
    "    print(f\"The table {table_name} exists.\")\n",
    "else:\n",
    "    print(f\"The table does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
